{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNfN18Jk//SnbWO/Nh0IXSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekonishi8524/my-colab-notebooks/blob/main/soseki_gpt_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertJapaneseTokenizer"
      ],
      "metadata": {
        "id": "lsvOxsF2XNg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "\n",
        "def load_aozora_text(url):\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'shift_jis'\n",
        "    text = response.text\n",
        "\n",
        "    parts = re.split(r'\\-{5,}', text)\n",
        "\n",
        "    if len(parts) > 2:\n",
        "        main_content = parts[2]\n",
        "    else:\n",
        "        main_content = text\n",
        "\n",
        "    main_content = re.split(r'底本 : ', main_content)[0]\n",
        "    cleaned_text = main_content\n",
        "\n",
        "    cleaned_text = re.sub(r' 《.*?》', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'［.*?］', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'<[^>]*?>', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\r\\n|\\r|\\n', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'　', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r' ', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'[^\\u3040-\\u30ff\\u3400-\\u4DBF\\u4E00\\\n",
        "    -\\u9FFF\\uF900-\\uFAFF\\uFF00-\\uFFEF\\u3000-\\u300F\\u30FB-\\u30FC]',\\\n",
        "                          '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'（.*?）', '', cleaned_text)\n",
        "\n",
        "    print(f\"[DEBUG] Length of cleaned text data: {len(cleaned_text)}\\\n",
        "     characters\")\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "XSAQY3GY7oDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class TransformerDecoderOnlyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, \\\n",
        "                 max_seq_len=64, dropout=0.1):\n",
        "        super(TransformerDecoderOnlyModel, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_len)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads,\\\n",
        "                                                   dim_feedforward=d_model*4,\\\n",
        "                                                   dropout=dropout,\\\n",
        "                                                   batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer,\\\n",
        "                                                         num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, tgt_mask):\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "\n",
        "        output = self.transformer_decoder(tgt, memory=tgt, tgt_mask=tgt_mask,\\\n",
        "                                          memory_mask=tgt_mask)\n",
        "        output = self.fc_out(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "wTgO6B279Jzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * \\\n",
        "         (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.permute(1, 0, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "d3lBfk4D9TRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample_text(model, tokenizer, prompt_text, device,\\\n",
        "                         max_length_gen=30, temperature=0.7, top_k=50):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt_text, add_special_tokens=False, \\\n",
        "                       return_tensors=\"pt\").to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    for _ in range(max_length_gen):\n",
        "        with torch.no_grad():\n",
        "           current_length = generated_ids.size(1)\n",
        "           tgt_mask = nn.Transformer.generate_square_subsequent_mask\\\n",
        "            (current_length).to(device)\n",
        "\n",
        "           outputs = model(generated_ids, tgt_mask)\n",
        "           next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "           if temperature > 0:\n",
        "               next_token_logits = next_token_logits / temperature\n",
        "\n",
        "           if top_k > 0:\n",
        "               top_k_value = min(top_k, next_token_logits.size(-1))\n",
        "               indices_to_remove = next_token_logits < \\\n",
        "               torch.topk(next_token_logits, top_k_value)[0][..., -1, None]\n",
        "               next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "           probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "           next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "           generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "\n",
        "    output_text = tokenizer.decode(generated_ids.squeeze(0),\\\n",
        "                                   skip_special_tokens=True)\n",
        "    model.train()\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "1616EjZoFs_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(\"Loading data...\")\n",
        "\n",
        "  try:\n",
        "      import fugashi\n",
        "  except ImportError:\n",
        "      !pip install fugashi\n",
        "      import fugashi\n",
        "\n",
        "  try:\n",
        "      import unidic_lite\n",
        "  except ImportError:\n",
        "      !pip install unidic-lite\n",
        "      import unidic_lite\n",
        "\n",
        "  aozora_urls = [\n",
        "      \"https://www.aozora.gr.jp/cards/000148/files/773_14560.html\", # こころ\n",
        "      \"https://www.aozora.gr.jp/cards/000148/files/789_14547.html\", # 吾輩は猫である\n",
        "      \"https://www.aozora.gr.jp/cards/000148/files/794_14946.html\", # 三四郎\n",
        "      \"https://www.aozora.gr.jp/cards/000148/files/56143_50921.html\", # それから\n",
        "      \"https://www.aozora.gr.jp/cards/000148/files/785_14971.html\" # 門\n",
        "  ]\n",
        "\n",
        "  tokenizer = BertJapaneseTokenizer.from_pretrained\\\n",
        "   ('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "  MAX_LENGTH = 64\n",
        "  STRIDE = 64\n",
        "\n",
        "  all_input_chunks = []\n",
        "\n",
        "  print(\"Loading and processing data from multiple URLs...\")\n",
        "  for url in aozora_urls:\n",
        "      try:\n",
        "          text = load_aozora_text(url)\n",
        "          print(f\"Processing {len(text)} characters from {url}\")\n",
        "\n",
        "          full_encoded = tokenizer(text, return_tensors='pt',\\\n",
        "                                   truncation=False, padding=False)\n",
        "          input_ids_file = full_encoded['input_ids'].squeeze(0)\n",
        "\n",
        "          for i in range(0, len(input_ids_file) - MAX_LENGTH + 1, STRIDE):\n",
        "              chunk = input_ids_file[i:i + MAX_LENGTH]\n",
        "              all_input_chunks.append(chunk)\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "  input_ids_tensor = torch.stack(all_input_chunks)\n",
        "\n",
        "  input_ids = input_ids_tensor[:, :-1]\n",
        "  labels = input_ids_tensor[:, 1:]\n",
        "\n",
        "  train_dataset = TensorDataset(input_ids, labels)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "  print(f\"[DEBUG] Total number of training samples (dataset size):\\\n",
        "   {len(train_dataset)}\")\n",
        "  print(f\"[DEBUG] Number of batches in DataLoader: {len(train_loader)}\")\n",
        "\n",
        "  VOCAB_SIZE = tokenizer.vocab_size\n",
        "  D_MODEL = 256\n",
        "  NUM_HEADS = 8\n",
        "  NUM_LAYERS = 4\n",
        "  MAX_LENGTH = 64\n",
        "\n",
        "  model = TransformerDecoderOnlyModel(VOCAB_SIZE, D_MODEL, NUM_HEADS,\\\n",
        "                                      NUM_LAYERS, max_seq_len=MAX_LENGTH)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  print(\"Starting training...\")\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  NUM_EPOCHS = 200\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader),\\\n",
        "                                               desc=f\"Epoch\\\n",
        "                                                {epoch+1}/{NUM_EPOCHS}\"):\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          tgt_mask = nn.Transformer.generate_square_subsequent_mask\\\n",
        "           (inputs.size(1)).to(device)\n",
        "          outputs = model(inputs, tgt_mask)\n",
        "          outputs = outputs.permute(0, 2, 1)\n",
        "\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {avg_loss:.4f}\")\n",
        "      sample_prompt = \"先生は私に\"\n",
        "      generated_sample = generate_sample_text(model, tokenizer,\\\n",
        "                                              sample_prompt, device)\n",
        "      print(f\"Output: {generated_sample}\\n\")\n",
        "  print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "U2ZlsbGVbB36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}